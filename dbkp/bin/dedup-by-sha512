#!/usr/bin/env python3

import os
import hashlib
from collections import defaultdict


def compute_hash(filepath):
    """Compute SHA512 hash of a file."""
    hash_sha512 = hashlib.sha512()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_sha512.update(chunk)
    return hash_sha512.hexdigest()


def find_and_remove_duplicates(directory):
    """Find and remove duplicate files, keeping the one with the shortest name."""
    files_by_hash = defaultdict(list)

    # Walk through all files and group them by hash
    for filename in os.listdir(directory):
        filepath = os.path.join(directory, filename)
        if os.path.isfile(filepath):
            file_hash = compute_hash(filepath)
            files_by_hash[file_hash].append(filepath)

    # For each group of files with the same hash, keep the shortest-named file
    for _, files in files_by_hash.items():
        if len(files) > 1:
            files.sort(key=lambda x: (len(os.path.basename(x)), x))
            shortest = files[0]
            to_remove = files[1:]
            print(f"Keeping: {shortest}")
            for filepath in to_remove:
                print(f"Removing: {filepath}")
                os.remove(filepath)


if __name__ == "__main__":
    current_directory = os.getcwd()
    find_and_remove_duplicates(current_directory)
